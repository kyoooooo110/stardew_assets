#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŸ æ˜Ÿéœ²è°·ç‰©è¯­Wikiå›¾ç‰‡æ‰¹é‡ä¸‹è½½ç¥å™¨

ä¸“ä¸ºå°çº¢ä¹¦åˆ›ä½œè€…å’Œæ¸¸æˆçˆ±å¥½è€…æ‰“é€ ï¼
ä¸€é”®ä¸‹è½½æ‰€æœ‰æ¸¸æˆç´ æï¼Œè®©ä½ çš„åˆ›ä½œæ›´ç²¾å½© âœ¨

åŠŸèƒ½ç‰¹è‰²ï¼š
ğŸ® è‡ªåŠ¨è·å–æ‰€æœ‰å›¾ç‰‡åˆ†ç±»
ğŸ“ æ™ºèƒ½æ•´ç†æ–‡ä»¶å¤¹ç»“æ„  
âš¡ å¤šçº¿ç¨‹é«˜é€Ÿä¸‹è½½
ğŸ”„ æ–­ç‚¹ç»­ä¼ ä¸ä¸¢å¤±
ğŸ’ é«˜æ¸…åŸå›¾è´¨é‡
"""

import os
import re
import time
import requests
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from pathlib import Path
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StardewImageScraper:
    """ğŸ® æ˜Ÿéœ²è°·ç‰©è¯­å›¾ç‰‡ä¸‹è½½ç¥å™¨
    
    ä¸“ä¸ºåˆ›ä½œè€…è®¾è®¡çš„æ™ºèƒ½ä¸‹è½½å·¥å…·ï¼Œè®©ç´ ææ”¶é›†å˜å¾—è¶…ç®€å•ï¼
    """
    
    def __init__(self, base_url="https://stardewvalleywiki.com", download_dir="stardew_images", max_workers=4):
        """ğŸš€ åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            base_url: WikiåŸºç¡€åœ°å€
            download_dir: ç´ æä¿å­˜æ–‡ä»¶å¤¹åç§°
            max_workers: ä¸‹è½½çº¿ç¨‹æ•°ï¼ˆé»˜è®¤2ä¸ªï¼Œæœ€å¤§4ä¸ªï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›ï¼‰
        """
        self.base_url = base_url
        self.download_dir = Path(download_dir)
        # é™åˆ¶æœ€å¤§çº¿ç¨‹æ•°ä¸º4ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›
        self.max_workers = min(max_workers, 4)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.downloaded_images = set()  # ğŸ¯ æ™ºèƒ½å»é‡ï¼Œé¿å…é‡å¤ä¸‹è½½
        self.processed_categories = set()  # ğŸ“‚ åˆ†ç±»è®°å½•ï¼Œæé«˜æ•ˆç‡
        self.lock = Lock()  # ğŸ”’ çº¿ç¨‹å®‰å…¨é”
        
        # ğŸ“ åˆ›å»ºç´ æå®åº“æ–‡ä»¶å¤¹
        self.download_dir.mkdir(exist_ok=True)
    
    def get_categories_from_web(self):
        """ğŸŒ ä»ç½‘ç»œæ™ºèƒ½è·å–æ‰€æœ‰å›¾ç‰‡åˆ†ç±»
        
        è¿™æ˜¯æ ¸å¿ƒåŠŸèƒ½ï¼è‡ªåŠ¨å‘ç°æ‰€æœ‰ç´ æåˆ†ç±»ï¼Œæ— éœ€æ‰‹åŠ¨æ“ä½œ âœ¨
        """
        images_category_url = f"{self.base_url}/Category:Images"
        logger.info(f"ğŸ” æ­£åœ¨è·å–åˆ†ç±»ä¿¡æ¯: {images_category_url}")
        
        content = self.get_page_content(images_category_url)
        if not content:
            logger.error("âŒ æ— æ³•è·å–åˆ†ç±»é¡µé¢å†…å®¹")
            return []
        
        return self.extract_subcategories_from_content(content)
    
    def parse_local_html(self, html_file_path):
        """è§£ææœ¬åœ°HTMLæ–‡ä»¶ï¼Œæå–æ‰€æœ‰å­åˆ†ç±»é“¾æ¥ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        with open(html_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return self.extract_subcategories_from_content(content)
    
    def extract_subcategories_from_content(self, content):
        """ä»HTMLå†…å®¹ä¸­æå–å­åˆ†ç±»é“¾æ¥"""
        soup = BeautifulSoup(content, 'html.parser')
        
        # æŸ¥æ‰¾Subcategorieséƒ¨åˆ†
        subcategories_section = soup.find('div', id='mw-subcategories')
        if not subcategories_section:
            return []
        
        # æå–æ‰€æœ‰åˆ†ç±»é“¾æ¥
        category_links = []
        for link in subcategories_section.find_all('a', href=True):
            href = link['href']
            if href.startswith('/Category:'):
                full_url = urljoin(self.base_url, href)
                category_name = link.get_text().strip()
                category_links.append({
                    'name': category_name,
                    'url': full_url,
                    'path': href
                })
        
        return category_links
    
    def get_page_content(self, url):
        """è·å–ç½‘é¡µå†…å®¹"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            logger.error(f"è·å–é¡µé¢å¤±è´¥ {url}: {e}")
            return None
    

    
    def get_original_image_url(self, thumbnail_url):
        """ä»ç¼©ç•¥å›¾URLè·å–åŸå§‹å›¾ç‰‡URL"""
        # ç§»é™¤ç¼©ç•¥å›¾å‚æ•°ï¼Œå¦‚ /120px-filename.png -> /filename.png
        if '/thumb/' in thumbnail_url:
            # å¤„ç†ç¼©ç•¥å›¾URLæ ¼å¼: /mediawiki/images/thumb/x/xx/filename.png/120px-filename.png
            parts = thumbnail_url.split('/thumb/')
            if len(parts) == 2:
                # æå–åŸå§‹è·¯å¾„
                thumb_part = parts[1]
                # æ‰¾åˆ°æœ€åä¸€ä¸ª/ä¹‹å‰çš„éƒ¨åˆ†
                last_slash = thumb_part.rfind('/')
                if last_slash != -1:
                    original_path = thumb_part[:last_slash]
                    return f"/mediawiki/images/{original_path}"
        
        return thumbnail_url
    
    def extract_filename_from_url(self, url):
        """ä»URLæå–æ–‡ä»¶å"""
        parsed = urlparse(url)
        filename = os.path.basename(parsed.path)
        return filename
    
    def download_image(self, image_info, category_dir):
        """ä¸‹è½½å•å¼ å›¾ç‰‡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        url = image_info['url']
        filename = image_info['filename']
        
        # çº¿ç¨‹å®‰å…¨åœ°æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        with self.lock:
            if url in self.downloaded_images:
                return True
        
        file_path = category_dir / filename
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡
        if file_path.exists():
            with self.lock:
                self.downloaded_images.add(url)
            return True
        
        try:
            # åˆ›å»ºæ–°çš„sessionå®ä¾‹ä»¥é¿å…çº¿ç¨‹å†²çª
            session = requests.Session()
            session.headers.update(self.session.headers)
            
            response = session.get(url, timeout=30)
            response.raise_for_status()
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
            
            with self.lock:
                self.downloaded_images.add(url)
            
            logger.info(f"ä¸‹è½½æˆåŠŸ: {filename}")
            return True
            
        except requests.RequestException as e:
            logger.error(f"ä¸‹è½½å¤±è´¥ {filename}: {e}")
            return False
    
    def process_category_recursive(self, category_info, parent_dir=None, depth=0, max_depth=10):
        """é€’å½’å¤„ç†åˆ†ç±»ï¼ŒåŒ…æ‹¬å­åˆ†ç±»ï¼ŒæŒ‰ç…§æ ‘çŠ¶ç»“æ„ç»„ç»‡ç›®å½•"""
        category_name = category_info['name']
        category_url = category_info['url']
        
        # é¿å…é‡å¤å¤„ç†å’Œæ— é™é€’å½’
        with self.lock:
            if category_url in self.processed_categories or depth > max_depth:
                return
            self.processed_categories.add(category_url)
        
        indent = "  " * depth
        logger.info(f"{indent}å¤„ç†åˆ†ç±»: {category_name} (æ·±åº¦: {depth})")
        
        # è·å–åˆ†ç±»é¡µé¢å†…å®¹
        content = self.get_page_content(category_url)
        if not content:
            return
        
        # åˆ›å»ºåˆ†ç±»ç›®å½• - æŒ‰ç…§æ ‘çŠ¶ç»“æ„ç»„ç»‡
        safe_name = re.sub(r'[<>:"/\\|?*]', '_', category_name)
        if parent_dir is None:
            # é¡¶çº§åˆ†ç±»ï¼Œç›´æ¥åœ¨ä¸‹è½½ç›®å½•ä¸‹åˆ›å»º
            category_dir = self.download_dir / safe_name
        else:
            # å­åˆ†ç±»ï¼Œåœ¨çˆ¶åˆ†ç±»ç›®å½•ä¸‹åˆ›å»º
            category_dir = parent_dir / safe_name
        
        category_dir.mkdir(exist_ok=True)
        
        # æå–å¹¶ä¸‹è½½å›¾ç‰‡
        images = self.extract_images_from_category_content(content)
        if images:
            logger.info(f"{indent}æ‰¾åˆ° {len(images)} å¼ å›¾ç‰‡ï¼Œå¼€å§‹å¹¶å‘ä¸‹è½½")
            self.download_images_concurrent(images, category_dir)
        
        # é€’å½’å¤„ç†å­åˆ†ç±»ï¼Œä¼ é€’å½“å‰åˆ†ç±»ç›®å½•ä½œä¸ºçˆ¶ç›®å½•
        subcategories = self.extract_subcategories_from_content(content)
        if subcategories:
            logger.info(f"{indent}æ‰¾åˆ° {len(subcategories)} ä¸ªå­åˆ†ç±»")
            for subcategory in subcategories:
                self.process_category_recursive(subcategory, category_dir, depth + 1, max_depth)
                time.sleep(0.2)  # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
    
    def extract_images_from_category_content(self, content):
        """ä»åˆ†ç±»é¡µé¢å†…å®¹ä¸­æå–å›¾ç‰‡é“¾æ¥"""
        soup = BeautifulSoup(content, 'html.parser')
        images = []
        
        # æŸ¥æ‰¾Media in categoryéƒ¨åˆ†
        media_section = soup.find('div', id='mw-category-media')
        if media_section:
            # æå–galleryä¸­çš„å›¾ç‰‡
            gallery = media_section.find('ul', class_='gallery')
            if gallery:
                for img_tag in gallery.find_all('img'):
                    src = img_tag.get('src')
                    if src:
                        # è·å–åŸå§‹å›¾ç‰‡URLï¼ˆå»æ‰ç¼©ç•¥å›¾å‚æ•°ï¼‰
                        original_src = self.get_original_image_url(src)
                        full_url = urljoin(self.base_url, original_src)
                        
                        # è·å–å›¾ç‰‡æ–‡ä»¶å
                        filename = self.extract_filename_from_url(full_url)
                        
                        images.append({
                            'url': full_url,
                            'filename': filename,
                            'alt': img_tag.get('alt', '')
                        })
        
        return images
    
    def download_images_concurrent(self, images, category_dir):
        """å¹¶å‘ä¸‹è½½å›¾ç‰‡"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤ä¸‹è½½ä»»åŠ¡
            future_to_image = {executor.submit(self.download_image, image_info, category_dir): image_info 
                             for image_info in images}
            
            success_count = 0
            for future in as_completed(future_to_image):
                image_info = future_to_image[future]
                try:
                    if future.result():
                        success_count += 1
                except Exception as exc:
                    logger.error(f"ä¸‹è½½å›¾ç‰‡ {image_info['filename']} æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")
            
            logger.info(f"å¹¶å‘ä¸‹è½½å®Œæˆï¼ŒæˆåŠŸä¸‹è½½ {success_count}/{len(images)} å¼ å›¾ç‰‡")
    
    def download_specific_category(self, category_url, category_name=None):
        """ğŸ¯ ä¸‹è½½æŒ‡å®šç±»åˆ«çš„å›¾ç‰‡
        
        Args:
            category_url: æŒ‡å®šçš„ç±»åˆ«URL
            category_name: ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›ä¼šä»URLä¸­æå–ï¼‰
        """
        logger.info(f"ğŸ¯ å¼€å§‹ä¸‹è½½æŒ‡å®šç±»åˆ«: {category_url}")
        
        # å¦‚æœæ²¡æœ‰æä¾›ç±»åˆ«åç§°ï¼Œä»URLä¸­æå–
        if not category_name:
            if '/Category:' in category_url:
                category_name = category_url.split('/Category:')[-1].replace('_', ' ')
            else:
                category_name = "SpecificCategory"
        
        # æ„é€ ç±»åˆ«ä¿¡æ¯
        category_info = {
            'name': category_name,
            'url': category_url,
            'path': category_url.replace(self.base_url, '')
        }
        
        # å¤„ç†æŒ‡å®šç±»åˆ«
        self.process_category_recursive(category_info, parent_dir=None)
        
        logger.info(f"\næŒ‡å®šç±»åˆ«ä¸‹è½½å®Œæˆï¼")
        logger.info(f"å›¾ç‰‡ä¸‹è½½åˆ°ç›®å½•: {self.download_dir.absolute()}")
        logger.info(f"æ€»å…±å¤„ç†äº† {len(self.processed_categories)} ä¸ªåˆ†ç±»")
        logger.info(f"æ€»å…±ä¸‹è½½äº† {len(self.downloaded_images)} å¼ å›¾ç‰‡")
    
    def run(self, html_file_path=None, specific_category_url=None, category_name=None):
        """ä¸»è¿è¡Œå‡½æ•°
        
        Args:
            html_file_path: æœ¬åœ°HTMLæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            specific_category_url: æŒ‡å®šç±»åˆ«URLï¼ˆå¯é€‰ï¼Œç”¨äºæŒ‰éœ€ä¸‹è½½ï¼‰
            category_name: æŒ‡å®šç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰
        """
        logger.info(f"å¼€å§‹è§£æStardew Valley Wikiå›¾ç‰‡ (å¹¶å‘æ•°: {self.max_workers})")
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šç±»åˆ«URLï¼Œåªä¸‹è½½è¯¥ç±»åˆ«
        if specific_category_url:
            self.download_specific_category(specific_category_url, category_name)
            return
        
        # è·å–æ‰€æœ‰åˆ†ç±»
        if html_file_path and os.path.exists(html_file_path):
            logger.info(f"ä½¿ç”¨æœ¬åœ°HTMLæ–‡ä»¶: {html_file_path}")
            categories = self.parse_local_html(html_file_path)
        else:
            logger.info("ä»ç½‘ç»œè·å–åˆ†ç±»ä¿¡æ¯")
            categories = self.get_categories_from_web()
        
        if not categories:
            logger.error("æœªæ‰¾åˆ°ä»»ä½•åˆ†ç±»")
            return
        
        logger.info(f"æ‰¾åˆ° {len(categories)} ä¸ªé¡¶çº§åˆ†ç±»ï¼Œå¼€å§‹é€’å½’å¤„ç†")
        
        # é€’å½’å¤„ç†æ¯ä¸ªåˆ†ç±»
        for i, category in enumerate(categories, 1):
            logger.info(f"\n=== è¿›åº¦: {i}/{len(categories)} ===")
            self.process_category_recursive(category, parent_dir=None)
            time.sleep(0.5)  # åˆ†ç±»ä¹‹é—´çš„å»¶è¿Ÿ
        
        logger.info("\næ‰€æœ‰åˆ†ç±»å¤„ç†å®Œæˆï¼")
        logger.info(f"å›¾ç‰‡ä¸‹è½½åˆ°ç›®å½•: {self.download_dir.absolute()}")
        logger.info(f"æ€»å…±å¤„ç†äº† {len(self.processed_categories)} ä¸ªåˆ†ç±»")
        logger.info(f"æ€»å…±ä¸‹è½½äº† {len(self.downloaded_images)} å¼ å›¾ç‰‡")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ğŸŒŸ æ˜Ÿéœ²è°·ç‰©è¯­Wikiå›¾ç‰‡æ‰¹é‡ä¸‹è½½ç¥å™¨')
    parser.add_argument('--category-url', '-u', help='æŒ‡å®šç±»åˆ«URLè¿›è¡ŒæŒ‰éœ€ä¸‹è½½')
    parser.add_argument('--category-name', '-n', help='æŒ‡å®šç±»åˆ«åç§°ï¼ˆé…åˆ--category-urlä½¿ç”¨ï¼‰')
    parser.add_argument('--max-workers', '-w', type=int, default=2, help='æœ€å¤§çº¿ç¨‹æ•°ï¼ˆé»˜è®¤2ï¼Œæœ€å¤§4ï¼‰')
    parser.add_argument('--download-dir', '-d', default='stardew_images', help='ä¸‹è½½ç›®å½•ï¼ˆé»˜è®¤stardew_imagesï¼‰')
    parser.add_argument('--html-file', '-f', help='ä½¿ç”¨æœ¬åœ°HTMLæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼Œé»˜è®¤å¹¶å‘æ•°ä¸º4
    scraper = StardewImageScraper(
        download_dir=args.download_dir, 
        max_workers=args.max_workers
    )
    
    # è¿è¡Œçˆ¬è™«
    if args.category_url:
        # æŒ‰éœ€ä¸‹è½½æŒ‡å®šç±»åˆ«
        logger.info(f"ğŸ¯ æŒ‰éœ€ä¸‹è½½æ¨¡å¼ï¼š{args.category_url}")
        scraper.download_specific_category(args.category_url, args.category_name)
    else:
        # å…¨é‡ä¸‹è½½æˆ–ä½¿ç”¨æœ¬åœ°HTMLæ–‡ä»¶
        scraper.run(html_file_path=args.html_file)
    
    logger.info("\nâœ¨ ä½¿ç”¨ç¤ºä¾‹:")
    logger.info("å…¨é‡ä¸‹è½½: python stardew_image_scraper.py")
    logger.info("æŒ‰éœ€ä¸‹è½½: python stardew_image_scraper.py -u 'https://stardewvalleywiki.com/Category:Achievement_images' -n 'æˆå°±å›¾æ ‡'")
    logger.info("è‡ªå®šä¹‰çº¿ç¨‹æ•°: python stardew_image_scraper.py -w 2")
    logger.info("ğŸ’¡ å»ºè®®ä½¿ç”¨æŒ‰éœ€ä¸‹è½½ï¼Œé¿å…å¯¹æœåŠ¡å™¨é€ æˆè¿‡å¤§å‹åŠ›ï¼")

if __name__ == "__main__":
    main()